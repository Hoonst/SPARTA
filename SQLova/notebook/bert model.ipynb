{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87064ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T13:46:58.841685Z",
     "start_time": "2021-04-15T13:46:58.611372Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "from train import get_data, get_opt, construct_hyper_param, get_models\n",
    "from sqlova.utils.utils_wikisql import get_fields, get_g, get_g_wvi_corenlp, get_wemb_bert, \\\n",
    "                                       get_g_wvi_bert_from_g_wvi_corenlp, generate_inputs\n",
    "import argparse\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65bb2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c8a564f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-type: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = construct_hyper_param(parser, notebook=True)\n",
    "args.bS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a46e4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_h = '../data/ko_token_not_h' \n",
    "path_wikisql = '../data/ko_token_not_h'  \n",
    "BERT_PT_PATH = path_wikisql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120cda2",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ee0612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_table, dev_data, dev_table, train_loader, dev_loader = get_data(path_wikisql, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5025a",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8628435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_bert = '../model_bert_best.pt'\n",
    "path_model = '../model_best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81118fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.bert_name = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3468001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size = 8\n",
      "BERT parameters:\n",
      "learning rate: 1e-05\n",
      "Fine-tune BERT: False\n",
      "Seq-to-SQL: the number of final BERT layers to be used: 2\n",
      "Seq-to-SQL: the size of hidden dimension = 100\n",
      "Seq-to-SQL: LSTM encoding layer size = 2\n",
      "Seq-to-SQL: dropout rate = 0.3\n",
      "Seq-to-SQL: learning rate = 0.001\n"
     ]
    }
   ],
   "source": [
    "# To start from the pre-trained models, un-comment following lines.\n",
    "\n",
    "model, model_bert, tokenizer, bert_config = get_models(args, BERT_PT_PATH, trained=False,\n",
    "                                                       path_model_bert=path_model_bert, path_model=path_model,\n",
    "                                                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "584390bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt, opt_bert = get_opt(args, model, model_bert, args.fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b516bc",
   "metadata": {},
   "source": [
    "# Train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03091bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dfb97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3273d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5f2d5355cf41afaf79b26c7f016538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1053.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "IDX: 972",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f46035d6ea36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0msegment_ids1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'IDX: {idx}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_mask1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_ids1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: IDX: 972"
     ]
    }
   ],
   "source": [
    "for idx, t in enumerate(tqdm(dev_loader)):\n",
    "    nlu, nlu_t, sql_i, sql_q, sql_t, tb, hs_t, hds = get_fields(t, dev_table, no_hs_t=True, no_sql_t=True)\n",
    "\n",
    "    l_n = []\n",
    "    l_hs = []  # The length of columns for each batch\n",
    "\n",
    "    input_ids = []\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "\n",
    "    i_nlu = []  # index to retreive the position of contextual vector later.\n",
    "    i_hds = []\n",
    "\n",
    "    doc_tokens = []\n",
    "    nlu_tt = []\n",
    "\n",
    "    t_to_tt_idx = []\n",
    "    tt_to_t_idx = []\n",
    "\n",
    "    for b, nlu_t1 in enumerate(nlu_t):\n",
    "\n",
    "        hds1 = hds[b]\n",
    "        l_hs.append(len(hds1))\n",
    "\n",
    "\n",
    "        # 1. 2nd tokenization using WordPiece\n",
    "        tt_to_t_idx1 = []  # number indicates where sub-token belongs to in 1st-level-tokens (here, CoreNLP).\n",
    "        t_to_tt_idx1 = []  # orig_to_tok_idx[i] = start index of i-th-1st-level-token in all_tokens.\n",
    "        nlu_tt1 = []  # all_doc_tokens[ orig_to_tok_idx[i] ] returns first sub-token segement of i-th-1st-level-token\n",
    "        for (i, token) in enumerate(nlu_t1):\n",
    "            t_to_tt_idx1.append(\n",
    "                len(nlu_tt1))  # all_doc_tokens[ indicate the start position of original 'white-space' tokens.\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tt_to_t_idx1.append(i)\n",
    "                nlu_tt1.append(sub_token)  # all_doc_tokens are further tokenized using WordPiece tokenizer\n",
    "        nlu_tt.append(nlu_tt1)\n",
    "        tt_to_t_idx.append(tt_to_t_idx1)\n",
    "        t_to_tt_idx.append(t_to_tt_idx1)\n",
    "\n",
    "        l_n.append(len(nlu_tt1))\n",
    "        #         hds1_all_tok = tokenize_hds1(tokenizer, hds1)\n",
    "\n",
    "\n",
    "\n",
    "        # [CLS] nlu [SEP] col1 [SEP] col2 [SEP] ...col-n [SEP]\n",
    "        # 2. Generate BERT inputs & indices.\n",
    "        tokens1, segment_ids1, i_nlu1, i_hds1 = generate_inputs(tokenizer, nlu_tt1, hds1)\n",
    "        input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "        # Input masks\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask1 = [1] * len(input_ids1)\n",
    "\n",
    "        # 3. Zero-pad up to the sequence length.\n",
    "        while len(input_ids1) < args.max_seq_length:\n",
    "            input_ids1.append(0)\n",
    "            input_mask1.append(0)\n",
    "            segment_ids1.append(0)\n",
    "\n",
    "        assert len(input_ids1) == args.max_seq_length, f'IDX: {idx}'\n",
    "        assert len(input_mask1) == args.max_seq_length\n",
    "        assert len(segment_ids1) == args.max_seq_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee0fb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, t in enumerate(dev_loader):\n",
    "    if idx == 972:\n",
    "        temp = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9eea40e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_id': '2-12394513-1',\n",
       "  'phase': 2,\n",
       "  'question': 'ketftv.com의 해상도와 kldo-dt2의 콜사인 이름',\n",
       "  'question_tok': ['ketftv',\n",
       "   '.',\n",
       "   'com',\n",
       "   '의',\n",
       "   '해상도',\n",
       "   '와',\n",
       "   'kldo',\n",
       "   '-',\n",
       "   'dt',\n",
       "   '2',\n",
       "   '의',\n",
       "   '콜사',\n",
       "   '인',\n",
       "   '이름'],\n",
       "  'sql': {'sel': 3,\n",
       "   'conds': [[5, 0, 'ketftv.com'], [1, 0, 'kldo-dt2']],\n",
       "   'agg': 0},\n",
       "  'query': {'sel': 3,\n",
       "   'conds': [[5, 0, 'ketftv.com'], [1, 0, 'kldo-dt2']],\n",
       "   'agg': 0},\n",
       "  'wvi_corenlp': [[0, 2], [6, 9]]},\n",
       " {'table_id': '2-12394513-1',\n",
       "  'phase': 2,\n",
       "  'question': '8126의 접시로 해상도 이름',\n",
       "  'question_tok': ['8126', '의', '접시', '로', '해상도', '이름'],\n",
       "  'sql': {'sel': 3, 'conds': [[0, 0, '8126']], 'agg': 0},\n",
       "  'query': {'sel': 3, 'conds': [[0, 0, '8126']], 'agg': 0},\n",
       "  'wvi_corenlp': [[0, 0]]},\n",
       " {'table_id': '2-1226461-1',\n",
       "  'phase': 2,\n",
       "  'question': 'brm p180을 섀시로 한 marlboro brm 팀의 최고 점수는?',\n",
       "  'question_tok': ['brm',\n",
       "   'p',\n",
       "   '180',\n",
       "   '을',\n",
       "   '섀시',\n",
       "   '로',\n",
       "   '한',\n",
       "   'marlboro',\n",
       "   'brm',\n",
       "   '팀',\n",
       "   '의',\n",
       "   '최고',\n",
       "   '점수',\n",
       "   '는',\n",
       "   '?'],\n",
       "  'sql': {'sel': 4,\n",
       "   'conds': [[1, 0, 'marlboro brm'], [2, 0, 'brm p180']],\n",
       "   'agg': 1},\n",
       "  'query': {'sel': 4,\n",
       "   'conds': [[1, 0, 'marlboro brm'], [2, 0, 'brm p180']],\n",
       "   'agg': 1},\n",
       "  'wvi_corenlp': [[7, 8], [0, 2]]},\n",
       " {'table_id': '2-1226461-1',\n",
       "  'phase': 2,\n",
       "  'question': 'marlboro brm이 팀으로있는 섀시는 무엇입니까?',\n",
       "  'question_tok': ['marlboro',\n",
       "   'brm',\n",
       "   '이',\n",
       "   '팀',\n",
       "   '으로',\n",
       "   '있',\n",
       "   '는',\n",
       "   '섀시',\n",
       "   '는',\n",
       "   '무엇',\n",
       "   '입니까',\n",
       "   '?'],\n",
       "  'sql': {'sel': 2, 'conds': [[1, 0, 'marlboro brm']], 'agg': 0},\n",
       "  'query': {'sel': 2, 'conds': [[1, 0, 'marlboro brm']], 'agg': 0},\n",
       "  'wvi_corenlp': [[0, 1]]},\n",
       " {'table_id': '2-1252114-1',\n",
       "  'phase': 2,\n",
       "  'question': 'Jimmy Reece는 12 랭크를 올렸을 때 어디에서 시작 했나요?',\n",
       "  'question_tok': ['jimmy',\n",
       "   'reece',\n",
       "   '는',\n",
       "   '12',\n",
       "   '랭크',\n",
       "   '를',\n",
       "   '올렸',\n",
       "   '을',\n",
       "   '때',\n",
       "   '어디',\n",
       "   '에서',\n",
       "   '시작',\n",
       "   '했',\n",
       "   '나요',\n",
       "   '?'],\n",
       "  'sql': {'sel': 1, 'conds': [[3, 0, '12']], 'agg': 0},\n",
       "  'query': {'sel': 1, 'conds': [[3, 0, '12']], 'agg': 0},\n",
       "  'wvi_corenlp': [[3, 3]]},\n",
       " {'table_id': '2-1252114-1',\n",
       "  'phase': 2,\n",
       "  'question': 'Jimmy Reece는 1957에서 어느 곳을 끝냈나요?',\n",
       "  'question_tok': ['jimmy',\n",
       "   'reece',\n",
       "   '는',\n",
       "   '1957',\n",
       "   '에서',\n",
       "   '어느',\n",
       "   '곳',\n",
       "   '을',\n",
       "   '끝냈',\n",
       "   '나요',\n",
       "   '?'],\n",
       "  'sql': {'sel': 4, 'conds': [[0, 0, '1957']], 'agg': 0},\n",
       "  'query': {'sel': 4, 'conds': [[0, 0, '1957']], 'agg': 0},\n",
       "  'wvi_corenlp': [[3, 3]]},\n",
       " {'table_id': '2-12230393-9',\n",
       "  'phase': 2,\n",
       "  'question': '애들레이드, 골드 코스트, 멜버른, 오클랜드가있는 시드니는 모두 yes  애들레이드, 골드 코스트, 멜버른, 오클랜드가있는 시드니는 애들레이드, 골드 코스트, 멜버른, 오클랜드가있는 시드니는 모두 yes  애들레이드가있는 시드니, 골드 코스트, 멜버른, 오클랜드가 모두 yes  애들레이드가있는 시드니입니다 , 골드 코스트, 멜버른, 오클랜드는 모두 yes  애들레이드가있는 시드니, 골드 코스트, 멜버른, 오클랜드가 모두 yes  애들레이드, 골드 코스트가있는 시드니입니다. , 멜버른, 오클랜드는 모두 yes  애들레이드가있는 시드니, 골드 코스트, 멜버른, 오클랜드는 모두 yes?',\n",
       "  'question_tok': ['애들레이드',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   '는',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   '는',\n",
       "   '애들레이드',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   '는',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '가',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   '입니다',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '는',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '가',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   '입니다',\n",
       "   '.',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '는',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '애들레이드',\n",
       "   '가',\n",
       "   '있',\n",
       "   '는',\n",
       "   '시드니',\n",
       "   ',',\n",
       "   '골드',\n",
       "   '코스트',\n",
       "   ',',\n",
       "   '멜버른',\n",
       "   ',',\n",
       "   '오클랜드',\n",
       "   '는',\n",
       "   '모두',\n",
       "   'yes',\n",
       "   '?'],\n",
       "  'sql': {'sel': 0,\n",
       "   'conds': [[3, 0, 'yes'], [4, 0, 'yes'], [1, 0, 'yes'], [5, 0, 'yes']],\n",
       "   'agg': 0},\n",
       "  'query': {'sel': 0,\n",
       "   'conds': [[3, 0, 'yes'], [4, 0, 'yes'], [1, 0, 'yes'], [5, 0, 'yes']],\n",
       "   'agg': 0},\n",
       "  'wvi_corenlp': [[14, 14], [14, 14], [14, 14], [14, 14]]},\n",
       " {'table_id': '2-12230393-9',\n",
       "  'phase': 2,\n",
       "  'question': 'No이있는 멜버른이란?-골드 코스트',\n",
       "  'question_tok': ['no',\n",
       "   '이',\n",
       "   '있',\n",
       "   '는',\n",
       "   '멜버른',\n",
       "   '이',\n",
       "   '란',\n",
       "   '?',\n",
       "   '-',\n",
       "   '골드',\n",
       "   '코스트'],\n",
       "  'sql': {'sel': 1, 'conds': [[4, 0, 'no']], 'agg': 0},\n",
       "  'query': {'sel': 1, 'conds': [[4, 0, 'no']], 'agg': 0},\n",
       "  'wvi_corenlp': [[0, 0]]}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da051a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlu_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-40e324da655a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mmodel_bert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     \u001b[0mnlu_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                     \u001b[0mhds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlu_t' is not defined"
     ]
    }
   ],
   "source": [
    "wemb_n, wemb_h, l_n, l_hpu, l_hs, \\\n",
    "nlu_tt, t_to_tt_idx, tt_to_t_idx \\\n",
    "                    = get_wemb_bert(bert_config, \n",
    "                                    model_bert, \n",
    "                                    tokenizer, \n",
    "                                    nlu_t, \n",
    "                                    hds, \n",
    "                                    max_seq_length,\n",
    "                                    num_out_layers_n=num_target_layers, \n",
    "                                    num_out_layers_h=num_target_layers, \n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38be57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
