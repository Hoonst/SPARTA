{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  2\n",
      "Current cuda device  0\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import torch\n",
    "from kobert_transformers import get_kobert_model\n",
    "from kobert_transformers import get_tokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# 현재 Setup 되어있는 device 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  1\n",
      "GeForce GTX 1080\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 1 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizers\n",
    "\n",
    "from kobert_transformers import get_tokenizer\n",
    "kobert_tokenizer = get_tokenizer()\n",
    "\n",
    "kor_tokenizer = kobert_tokenizer.from_pretrained('monologg/kobert')\n",
    "monologg_tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "eng_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "multi_uncased_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "multi_cased_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_example_sentence1 = \"[CLS] 우리 둘의 마지막 페이지를 잘 부탁해 [SEP]\"\n",
    "eng_example_sentence1 = \"[CLS] Please take care of the last page of both of us. [SEP]\"\n",
    "kor_eng_example_sentence1 = \"[CLS] Our 둘의 Last Page를 잘 부탁해 [SEP]\"\n",
    "\n",
    "kor_example_sentence2 = \"[CLS] 처음 만난 그날처럼 예쁘다고 말해줄래 [SEP]\"\n",
    "eng_example_sentence2 = \"[CLS] Can you tell me you're as pretty as the day we first met? [SEP]\"\n",
    "kor_eng_example_sentence2 = \"[CLS] 처음 만난 that day처럼 pretty 말해줄래 [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean Example Sentence1: [CLS] 우리 둘의 마지막 페이지를 잘 부탁해 [SEP]\n",
      "English Example Sentence1: [CLS] Please take care of the last page of both of us. [SEP]\n",
      "Kor & Eng Example Sentence1: [CLS] Our 둘의 Last Page를 잘 부탁해 [SEP]\n",
      "\n",
      "Korean Example Sentence2: [CLS] 처음 만난 그날처럼 예쁘다고 말해줄래 [SEP]\n",
      "English Example Sentence2: [CLS] Can you tell me you're as pretty as the day we first met? [SEP]\n",
      "Kor & Eng Example Sentence2: [CLS] 처음 만난 that day처럼 pretty 말해줄래 [SEP]\n",
      "\n",
      "[Kobert Tokenizer]\n",
      "Korean1 > Kobert tokenizer: \n",
      "['[CLS]', '▁우리', '▁둘', '의', '▁마지막', '▁페', '이', '지를', '▁잘', '▁부탁', '해', '[SEP]']\n",
      "Korean2 > Kobert tokenizer: \n",
      "['[CLS]', '▁처음', '▁만난', '▁그', '날', '처럼', '▁예쁘', '다', '고', '▁말해', '줄', '래', '[SEP]']\n",
      "Kor & Eng2 > Kobert tokenizer: \n",
      " ['[CLS]', '▁처음', '▁만난', '▁', 'th', 'at', '▁', 'd', 'ay', '처럼', '▁', 'p', 're', 't', 't', 'y', '▁말해', '줄', '래', '[SEP]']\n",
      "==================================================================================================== \n",
      "\n",
      "[monologg Tokenizer]\n",
      "Monologg Tokenizer doesn't work properly, needs Kobert Tokenizer to fix it\n",
      "Korean1 > monologg tokenizer: \n",
      "['[CLS]', '우리', '[UNK]', '[UNK]', '[UNK]', '잘', '[UNK]', '[SEP]']\n",
      "Korean2 > monologg tokenizer: \n",
      "['[CLS]', '처음', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
      "==================================================================================================== \n",
      "\n",
      "[Bert-base-uncased Tokenizer]\n",
      "English1 > English tokenizer: \n",
      "['[CLS]', 'please', 'take', 'care', 'of', 'the', 'last', 'page', 'of', 'both', 'of', 'us', '.', '[SEP]']\n",
      "English2 > English tokenizer: \n",
      "['[CLS]', 'can', 'you', 'tell', 'me', 'you', \"'\", 're', 'as', 'pretty', 'as', 'the', 'day', 'we', 'first', 'met', '?', '[SEP]']\n",
      "==================================================================================================== \n",
      "\n",
      "[Bert-base-uncased Tokenizer]\n",
      "Kor & Eng1 > English Tokenizer: \n",
      " ['[CLS]', 'our', 'ᄃ', '##ᅮ', '##ᆯ', '##ᄋ', '##ᅴ', 'last', 'page', '##ᄅ', '##ᅳ', '##ᆯ', 'ᄌ', '##ᅡ', '##ᆯ', 'ᄇ', '##ᅮ', '##ᄐ', '##ᅡ', '##ᆨ', '##ᄒ', '##ᅢ', '[SEP]']\n",
      "Kor & Eng2 > English Tokenizer: \n",
      " ['[CLS]', 'ᄎ', '##ᅥ', '##ᄋ', '##ᅳ', '##ᆷ', 'ᄆ', '##ᅡ', '##ᆫ', '##ᄂ', '##ᅡ', '##ᆫ', 'that', 'day', '##ᄎ', '##ᅥ', '##ᄅ', '##ᅥ', '##ᆷ', 'pretty', 'ᄆ', '##ᅡ', '##ᆯ', '##ᄒ', '##ᅢ', '##ᄌ', '##ᅮ', '##ᆯ', '##ᄅ', '##ᅢ', '[SEP]']\n",
      "==================================================================================================== \n",
      "\n",
      "[Bert-base-multilingual-uncased Tokenizer]\n",
      "Korean1 > monologg tokenizer: \n",
      "['[CLS]', 'ᄋ', '##ᅮ리', '두', '##ᆯ의', '마지막', 'ᄑ', '##ᅦ이지', '##를', '잘', 'ᄇ', '##ᅮ', '##타', '##ᆨ', '##해', '[SEP]']\n",
      "English1 > English tokenizer: \n",
      "['[CLS]', 'please', 'take', 'care', 'of', 'the', 'last', 'page', 'of', 'both', 'of', 'us', '.', '[SEP]']\n",
      "Kor & Eng1 > Multi (Uncased) Tokenizer: \n",
      " ['[CLS]', 'our', '두', '##ᆯ의', 'last', 'page', '##를', '잘', 'ᄇ', '##ᅮ', '##타', '##ᆨ', '##해', '[SEP]']\n",
      "Kor & Eng2 > Multi (Uncased) Tokenizer: \n",
      " ['[CLS]', '처음', 'ᄆ', '##ᅡᆫ', '##난', 'that', 'day', '##처럼', 'pretty', '말', '##해', '##주', '##ᆯ', '##래', '[SEP]']\n",
      "==================================================================================================== \n",
      "\n",
      "[Bert-base-multilingual-cased Tokenizer]\n",
      "Korean2 > monologg tokenizer: \n",
      "['[CLS]', '처음', '만', '##난', '그', '##날', '##처럼', '예', '##쁘', '##다고', '말', '##해', '##줄', '##래', '[SEP]']\n",
      "English2 > English tokenizer: \n",
      "['[CLS]', 'Can', 'you', 'tell', 'me', 'you', \"'\", 're', 'as', 'pretty', 'as', 'the', 'day', 'we', 'first', 'met', '?', '[SEP]']\n",
      "Kor & Eng1 > Multi (Cased) Tokenizer: \n",
      " ['[CLS]', 'Our', '둘', '##의', 'Last', 'Page', '##를', '잘', '부', '##탁', '##해', '[SEP]']\n",
      "Kor & Eng2 > Multi (Cased) Tokenizer: \n",
      " ['[CLS]', '처음', '만', '##난', 'that', 'day', '##처럼', 'pretty', '말', '##해', '##줄', '##래', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(f'Korean Example Sentence1: {kor_example_sentence1}')\n",
    "print(f'English Example Sentence1: {eng_example_sentence1}')\n",
    "print(f'Kor & Eng Example Sentence1: {kor_eng_example_sentence1}')\n",
    "print('')\n",
    "\n",
    "print(f'Korean Example Sentence2: {kor_example_sentence2}')\n",
    "print(f'English Example Sentence2: {eng_example_sentence2}')\n",
    "print(f'Kor & Eng Example Sentence2: {kor_eng_example_sentence2}')\n",
    "print('')\n",
    "\n",
    "print('[Kobert Tokenizer]')\n",
    "print(f'Korean1 > Kobert tokenizer: \\n{kor_tokenizer.tokenize(kor_example_sentence1)}')\n",
    "print(f'Korean2 > Kobert tokenizer: \\n{kor_tokenizer.tokenize(kor_example_sentence2)}')\n",
    "print(f'Kor & Eng2 > Kobert tokenizer: \\n {kor_tokenizer.tokenize(kor_eng_example_sentence2)}')\n",
    "print('='*100, '\\n')\n",
    "\n",
    "print('[monologg Tokenizer]')\n",
    "print(\"Monologg Tokenizer doesn't work properly, needs Kobert Tokenizer to fix it\")\n",
    "print(f'Korean1 > monologg tokenizer: \\n{monologg_tokenizer.tokenize(kor_example_sentence1)}')\n",
    "print(f'Korean2 > monologg tokenizer: \\n{monologg_tokenizer.tokenize(kor_example_sentence2)}')\n",
    "print('='*100, '\\n')\n",
    "\n",
    "print('[Bert-base-uncased Tokenizer]')\n",
    "print(f'English1 > English tokenizer: \\n{eng_tokenizer.tokenize(eng_example_sentence1)}')\n",
    "print(f'English2 > English tokenizer: \\n{eng_tokenizer.tokenize(eng_example_sentence2)}')\n",
    "print('='*100, '\\n')\n",
    "\n",
    "print('[Bert-base-uncased Tokenizer]')\n",
    "\n",
    "print(f'Kor & Eng1 > English Tokenizer: \\n {eng_tokenizer.tokenize(kor_eng_example_sentence1)}')\n",
    "print(f'Kor & Eng2 > English Tokenizer: \\n {eng_tokenizer.tokenize(kor_eng_example_sentence2)}')\n",
    "print('='*100, '\\n')\n",
    "\n",
    "print('[Bert-base-multilingual-uncased Tokenizer]')\n",
    "print(f'Korean1 > monologg tokenizer: \\n{multi_uncased_tokenizer.tokenize(kor_example_sentence1)}')\n",
    "print(f'English1 > English tokenizer: \\n{multi_uncased_tokenizer.tokenize(eng_example_sentence1)}')\n",
    "\n",
    "print(f'Kor & Eng1 > Multi (Uncased) Tokenizer: \\n {multi_uncased_tokenizer.tokenize(kor_eng_example_sentence1)}')\n",
    "print(f'Kor & Eng2 > Multi (Uncased) Tokenizer: \\n {multi_uncased_tokenizer.tokenize(kor_eng_example_sentence2)}')\n",
    "\n",
    "print('='*100, '\\n')\n",
    "\n",
    "print('[Bert-base-multilingual-cased Tokenizer]')\n",
    "print(f'Korean2 > monologg tokenizer: \\n{multi_cased_tokenizer.tokenize(kor_example_sentence2)}')\n",
    "print(f'English2 > English tokenizer: \\n{multi_cased_tokenizer.tokenize(eng_example_sentence2)}')\n",
    "\n",
    "print(f'Kor & Eng1 > Multi (Cased) Tokenizer: \\n {multi_cased_tokenizer.tokenize(kor_eng_example_sentence1)}')\n",
    "print(f'Kor & Eng2 > Multi (Cased) Tokenizer: \\n {multi_cased_tokenizer.tokenize(kor_eng_example_sentence2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('/repo/TabularSemanticParsing/data/wikisql1.1/dev.db')\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('johnny miller', 'united states', '1973', 282.0, 2.0, 't4')\n",
      "('hale irwin', 'united states', '1974 , 1979', 284.0, 4.0, '6')\n",
      "('lee trevino', 'united states', '1968 , 1971', 286.0, 6.0, 't9')\n",
      "('tom watson', 'united states', '1982', 287.0, 7.0, 't11')\n",
      "('david graham', 'australia', '1981', 287.0, 7.0, 't11')\n",
      "('jack nicklaus', 'united states', '1962 , 1967 , 1972 , 1980', 289.0, 9.0, 't21')\n",
      "('hubert green', 'united states', '1977', 291.0, 11.0, 't30')\n",
      "('gary player', 'south africa', '1965', 294.0, 14.0, 't43')\n"
     ]
    }
   ],
   "source": [
    "for row in cur.execute('SELECT * FROM table_2_17231267_1'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_wikisql1.1/wikisql.bridge.question-split.ppl-0.85.2.dn.no_from.bert.pkl', 'rb') as f:\n",
    "    engko_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_wikisql1.1/wikisql.bridge.question-split.ppl-0.85.2.dn.no_from.kobert.pkl', 'rb') as f:\n",
    "    ko_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/wikisql1.1/wikisql.bridge.question-split.ppl-0.85.2.dn.no_from.bert.pkl', 'rb') as f:\n",
    "    eng_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/multi_wikisql1.1/wikisql.bridge.question-split.ppl-0.85.2.dn.no_from.bert.multilingual.pkl', 'rb') as f:\n",
    "    mult_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL: b'South Australia\\xec\\x9d\\x98 notes\\xec\\x9d\\xb4 \\xeb\\xad\\x94\\xec\\xa7\\x80 \\xeb\\xa7\\x90\\xed\\x95\\xb4\\xec\\xa4\\x98'\n",
      "NL tokens: [b'South', b'Australia', b'##\\xec\\x9d\\x98', b'notes', b'##\\xec\\x9d\\xb4', b'\\xeb\\xad\\x94', b'##\\xec\\xa7\\x80', b'\\xeb\\xa7\\x90', b'##\\xed\\x95\\xb4', b'##\\xec\\xa4\\x98']\n",
      "NL tokens (original): [b'South', b'Australia', b'##\\xec\\x9d\\x98', b'notes', b'##\\xec\\x9d\\xb4', b'\\xeb\\xad\\x94', b'##\\xec\\xa7\\x80', b'\\xeb\\xa7\\x90', b'##\\xed\\x95\\xb4', b'##\\xec\\xa4\\x98']\n",
      "['[CLS]', 'South', 'Australia', '##의', 'notes', '##이', '뭔', '##지', '말', '##해', '##줘', '[SEP]', '[unused52]', '[unused50]', 'State', '/', 'territory', '[unused51]', 'State', '/', 'territory', '[unused49]', 'South', 'Australia', '[unused51]', 'Text', '/', 'background', 'colour', '[unused51]', 'Format', '[unused51]', 'Current', 'slogan', '[unused51]', 'Current', 'series', '[unused51]', 'Notes', '[SEP]']\n",
      "Target 0: b''\n",
      "b\"Target form: [{'sel': 5, 'conds': [[3, 0, 'SOUTH AUSTRALIA']], 'agg': 0}]\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mult_data['train'][0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/multi_wikisql1.1/wikisql.bridge.question-split.ppl-0.85.2.dn.no_from.eo.bert.multilingual.pkl', 'rb') as f:\n",
    "    multi_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL: b'South Australia\\xec\\x9d\\x98 notes\\xec\\x9d\\xb4 \\xeb\\xad\\x94\\xec\\xa7\\x80 \\xeb\\xa7\\x90\\xed\\x95\\xb4\\xec\\xa4\\x98'\n",
      "NL tokens: [b'South', b'Australia', b'##\\xec\\x9d\\x98', b'notes', b'##\\xec\\x9d\\xb4', b'\\xeb\\xad\\x94', b'##\\xec\\xa7\\x80', b'\\xeb\\xa7\\x90', b'##\\xed\\x95\\xb4', b'##\\xec\\xa4\\x98']\n",
      "NL tokens (original): [b'South', b'Australia', b'##\\xec\\x9d\\x98', b'notes', b'##\\xec\\x9d\\xb4', b'\\xeb\\xad\\x94', b'##\\xec\\xa7\\x80', b'\\xeb\\xa7\\x90', b'##\\xed\\x95\\xb4', b'##\\xec\\xa4\\x98']\n",
      "['[CLS]', 'South', 'Australia', '##의', 'notes', '##이', '뭔', '##지', '말', '##해', '##줘', '[SEP]', '[unused52]', '[unused50]', 'State', '/', 'territory', '[unused51]', 'State', '/', 'territory', '[unused49]', 'South', 'Australia', '[unused51]', 'Text', '/', 'background', 'colour', '[unused51]', 'Format', '[unused51]', 'Current', 'slogan', '[unused51]', 'Current', 'series', '[unused51]', 'Notes', '[SEP]']\n",
      "Target 0: b''\n",
      "b\"Target form: [{'sel': 5, 'conds': [[3, 0, 'SOUTH AUSTRALIA']], 'agg': 0}]\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_data['train'][0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50, 55):\n",
    "    print('#'*20)\n",
    "    print('-'*20)\n",
    "    print(\"English BERT\")\n",
    "    eng_data['train'][i].pretty_print()\n",
    "    print('-'*20)\n",
    "    print(\"Eng+Kor BERT\")\n",
    "    engko_data['train'][i].pretty_print()\n",
    "    print('-'*20)\n",
    "    print(\"Korean BERT\")\n",
    "    ko_data['train'][i].pretty_print()\n",
    "    print('-'*20)\n",
    "    print(\"Multi-Lingual BERT\")\n",
    "    multi_data['train'][i].pretty_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempts to resize the Embedding of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', '[UNK]', '[UNK]']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('내 몸이 너무 아파')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15529538fc1b4c50b241ac123ba8aeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7232b7ebfc5042f9b28761b48cb2ae11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45447fd10f314fb8ae368046cebb3f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=435779157, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "28996\n",
      "28997\n",
      "tensor([ 2.0105e-02,  4.1210e-02, -8.6391e-03, -1.2895e-02, -2.6548e-02,\n",
      "         1.2749e-02, -2.8585e-02,  2.9357e-02, -2.8315e-02, -1.6660e-03,\n",
      "        -2.8184e-02, -2.4176e-02, -1.1917e-02, -5.0954e-02, -5.3747e-03,\n",
      "        -2.3604e-02,  1.5609e-02,  8.0757e-03, -1.1517e-02, -8.6824e-03,\n",
      "         6.1110e-04,  1.4534e-02, -5.8289e-03,  1.3068e-02, -2.6396e-02,\n",
      "        -1.5126e-02, -4.6017e-03, -3.7856e-03, -3.0076e-02,  2.6408e-02,\n",
      "         4.2605e-03,  4.5580e-03, -1.6992e-02,  8.4462e-03,  1.6670e-02,\n",
      "        -7.7514e-03, -3.0544e-02,  1.1164e-02,  8.2609e-04, -9.2539e-03,\n",
      "         3.8989e-02, -2.1804e-02,  9.4779e-03,  1.6304e-03, -2.1960e-02,\n",
      "        -1.8996e-02,  2.2673e-02,  9.4029e-03,  5.8221e-03, -1.2463e-02,\n",
      "        -5.4119e-03,  2.4861e-02, -6.2257e-04,  4.0769e-02, -1.2550e-02,\n",
      "         1.2010e-02, -1.3524e-02, -1.0534e-02, -2.9601e-02,  1.2069e-02,\n",
      "        -2.6971e-02,  2.8995e-02,  7.6551e-03, -2.7225e-02, -2.7898e-02,\n",
      "         3.7611e-03, -1.0652e-02, -2.4479e-02, -2.2796e-02,  2.7228e-02,\n",
      "         1.3483e-02,  1.8584e-02, -6.1082e-04, -8.0583e-04,  6.4151e-03,\n",
      "         1.0712e-03,  6.6526e-03, -1.6035e-02,  1.5809e-02, -1.6755e-02,\n",
      "         2.8719e-02, -4.0017e-02,  3.2521e-02,  3.0280e-02,  3.6783e-03,\n",
      "         7.0912e-03, -5.9868e-03,  1.7179e-02, -1.8200e-02, -1.3692e-02,\n",
      "         1.0183e-02,  5.9831e-03,  2.1280e-02,  1.3371e-02, -2.2406e-02,\n",
      "        -1.6151e-02, -1.1078e-02,  1.9652e-02, -1.2140e-03,  7.7753e-03,\n",
      "        -3.0240e-02,  8.7024e-03,  2.4065e-02, -1.5370e-02, -8.5715e-04,\n",
      "        -8.5931e-03,  3.6466e-02,  2.2572e-02, -1.4723e-02, -6.2755e-03,\n",
      "         3.7843e-04, -1.2997e-02, -4.2351e-03,  1.5766e-02, -4.3190e-03,\n",
      "         1.9556e-02,  2.3832e-03,  2.6483e-03, -1.3928e-03,  9.6538e-03,\n",
      "        -8.0596e-03,  2.6131e-02, -1.4116e-02, -1.2676e-02, -2.1891e-02,\n",
      "        -1.3423e-02, -2.1934e-02, -2.0906e-02,  4.8412e-03,  1.4203e-02,\n",
      "         2.4164e-02,  1.6842e-02,  1.9085e-02, -3.5288e-02, -4.3413e-02,\n",
      "         2.2853e-02, -5.6426e-03,  5.1936e-03, -2.1006e-02,  3.6706e-02,\n",
      "         3.5954e-02,  1.5244e-02,  6.5930e-04, -8.0340e-03,  1.1706e-02,\n",
      "         9.2737e-03, -1.5013e-02,  4.2860e-04,  5.6152e-03,  2.4036e-04,\n",
      "        -1.2711e-02, -1.5728e-03, -1.6656e-02, -4.5815e-03,  3.6364e-02,\n",
      "         1.2689e-03,  2.8501e-02,  1.9906e-02,  3.6046e-02,  6.2852e-04,\n",
      "         1.6739e-02,  1.6218e-02, -2.6689e-02, -8.5047e-03, -1.0269e-02,\n",
      "         1.4069e-02,  1.3235e-02, -3.1891e-02,  2.8747e-02, -3.0126e-03,\n",
      "         1.5365e-02,  1.7203e-02,  1.0687e-02, -1.0593e-02,  1.8870e-02,\n",
      "        -1.2024e-02, -1.6801e-03, -1.9720e-02, -1.7710e-02, -1.4968e-02,\n",
      "        -1.0887e-03,  7.4627e-03, -6.1215e-03, -4.7897e-02,  3.1479e-02,\n",
      "         1.4061e-02,  2.0603e-02, -1.8247e-02, -9.6833e-03,  2.6633e-02,\n",
      "        -2.2312e-02, -4.7164e-02,  1.3462e-03,  2.2146e-03, -3.5903e-02,\n",
      "        -1.0072e-02,  9.7817e-03, -3.6105e-03,  1.1886e-02,  9.6432e-03,\n",
      "        -3.8040e-03, -2.8169e-02, -8.6554e-03,  1.7804e-02,  8.6695e-03,\n",
      "         3.3162e-02,  1.5678e-02,  2.1011e-02,  3.1902e-02, -9.9471e-03,\n",
      "        -3.1407e-02,  1.7446e-02,  6.0581e-02,  2.8391e-02,  2.2741e-02,\n",
      "         1.0508e-03, -5.8935e-03, -1.9877e-02,  4.4671e-03, -3.7090e-03,\n",
      "        -2.9904e-02,  1.9561e-02, -1.7120e-02, -1.6564e-02,  1.4909e-02,\n",
      "         7.9150e-03,  8.2177e-03,  8.9409e-03, -2.3303e-02, -3.5057e-03,\n",
      "         7.9698e-03,  9.7130e-03, -2.6581e-02,  2.0544e-02,  6.0624e-03,\n",
      "         7.1362e-03, -2.0047e-02,  3.2954e-02,  1.1842e-02,  6.8801e-03,\n",
      "         1.3682e-03, -1.2242e-02,  2.2680e-02, -2.9139e-02,  1.5723e-02,\n",
      "         6.4495e-03,  1.8747e-03, -2.3711e-02, -3.7229e-02,  3.5495e-02,\n",
      "        -1.9076e-02,  9.5600e-03,  2.4063e-02,  1.2249e-02, -2.0787e-02,\n",
      "        -1.2192e-02, -5.3649e-03, -2.3136e-02,  2.6611e-02, -8.5970e-03,\n",
      "        -1.1722e-02, -3.0182e-02, -9.9997e-04,  7.7162e-03,  3.0859e-02,\n",
      "        -1.5406e-02,  1.7070e-02,  7.4798e-03,  1.8491e-02, -2.8763e-03,\n",
      "        -5.1078e-03,  3.4242e-02, -1.5462e-02, -2.7981e-02, -1.5221e-02,\n",
      "         1.8886e-03,  3.9994e-03,  1.6642e-02,  1.4574e-02, -1.9021e-02,\n",
      "        -2.0481e-02,  5.7452e-03, -1.4432e-02, -2.5131e-02, -1.4610e-03,\n",
      "         5.3836e-03,  2.8631e-02, -1.5565e-02, -1.0009e-02,  8.0213e-04,\n",
      "         1.5970e-02,  3.4829e-03, -6.4747e-03, -9.1203e-04, -3.2740e-03,\n",
      "         4.0291e-03,  3.6663e-02, -3.0278e-02,  2.0908e-03, -5.8732e-03,\n",
      "         2.5121e-02,  1.7734e-02,  2.2035e-02,  3.4126e-03, -5.3198e-03,\n",
      "         7.6320e-03,  3.0501e-02, -4.4029e-02,  1.3455e-02, -6.2202e-03,\n",
      "        -3.8341e-03,  1.1159e-02, -2.1243e-02,  1.0810e-02,  1.9088e-02,\n",
      "        -2.8236e-03, -1.4453e-02,  2.1880e-02, -2.9822e-02, -8.4934e-03,\n",
      "         1.9353e-02, -4.3455e-03,  2.7521e-02,  6.2618e-03, -6.7535e-03,\n",
      "         1.6598e-02,  6.5077e-03,  1.0300e-02,  1.8147e-02,  3.1322e-02,\n",
      "        -4.2048e-02, -2.6558e-02,  2.2358e-02,  1.3632e-02, -1.2611e-03,\n",
      "        -3.6469e-02, -3.4290e-03,  2.2085e-02, -9.7856e-03,  4.7912e-02,\n",
      "        -5.6325e-04, -2.2646e-02, -1.7953e-02, -3.7431e-02, -4.4695e-03,\n",
      "        -4.0708e-03, -4.6796e-02, -5.4234e-03, -8.2857e-03,  1.2629e-02,\n",
      "        -4.0510e-02, -1.8105e-02,  2.8512e-02, -3.1696e-03,  6.1783e-03,\n",
      "         7.3461e-03,  3.0609e-02,  2.1668e-03, -7.7623e-04, -5.7273e-04,\n",
      "         1.5558e-02,  1.4367e-02, -3.6441e-03,  1.6789e-02, -1.3261e-02,\n",
      "        -1.8080e-02,  3.5845e-02,  3.0455e-02, -4.1081e-02, -2.6236e-02,\n",
      "         5.2693e-03,  4.9464e-02,  4.8095e-03,  1.1238e-02, -1.0646e-02,\n",
      "        -1.6814e-02,  1.1544e-02, -2.2407e-02,  5.2615e-03, -3.6361e-02,\n",
      "         5.0919e-03, -1.5940e-02, -1.4540e-02,  1.3205e-02,  7.0437e-03,\n",
      "         2.1815e-02, -7.7862e-03,  2.1611e-02, -1.1434e-02,  9.2925e-03,\n",
      "         2.0270e-02, -2.1426e-02,  8.7612e-03,  1.2808e-02, -2.8985e-03,\n",
      "         1.0348e-02, -2.3809e-02, -1.5695e-02,  4.9537e-03,  2.1832e-02,\n",
      "         2.9640e-02, -1.5375e-02,  1.9278e-02,  2.4163e-02,  2.7047e-02,\n",
      "        -1.2805e-02, -1.2608e-02,  2.8610e-03, -4.6654e-03, -1.3952e-02,\n",
      "        -2.9542e-02,  3.3213e-02,  5.9428e-04, -1.7584e-02,  3.5207e-02,\n",
      "        -2.1048e-02, -3.6015e-02, -3.2483e-02, -3.5836e-02,  8.6347e-03,\n",
      "        -7.5682e-04, -2.3290e-02,  9.2467e-03, -5.1569e-02,  1.9774e-04,\n",
      "        -2.1587e-02,  1.1000e-03, -1.8351e-03, -1.0747e-02,  3.3777e-02,\n",
      "        -1.9597e-02, -1.9487e-02,  1.9371e-02, -1.2348e-02,  4.6184e-03,\n",
      "         1.9033e-02,  1.3547e-02, -4.1312e-02,  1.3707e-02,  3.0773e-02,\n",
      "        -3.1927e-02,  1.1945e-02, -1.4838e-02, -2.2772e-02,  1.3962e-02,\n",
      "        -3.5912e-03,  1.7826e-02, -4.0260e-03, -1.3333e-02,  3.9665e-02,\n",
      "        -3.0875e-03, -1.5139e-02, -2.1106e-02,  2.5570e-03,  9.2277e-03,\n",
      "         1.6693e-02, -2.0054e-02,  6.2691e-03, -1.2723e-02, -8.7222e-03,\n",
      "        -1.3908e-02, -1.7883e-02, -1.3681e-03,  2.4283e-02,  1.8313e-02,\n",
      "        -2.6997e-02, -5.6459e-03,  1.2782e-02, -2.1768e-02,  7.6668e-03,\n",
      "         2.6432e-02,  3.8706e-02,  2.3358e-02, -7.5261e-03,  4.2073e-03,\n",
      "        -2.8182e-02, -2.4417e-02,  3.0876e-02, -1.7889e-03,  4.1029e-03,\n",
      "         4.8645e-03, -3.8856e-02,  3.5537e-03, -2.9993e-03,  2.1320e-02,\n",
      "        -2.6576e-02, -4.1301e-03,  2.7727e-02,  3.2920e-03, -4.3802e-02,\n",
      "         2.0154e-02,  3.3602e-02, -2.2277e-02, -1.1420e-02, -7.7387e-03,\n",
      "        -4.4954e-02,  5.4816e-02,  8.2440e-03, -4.6532e-02, -5.3978e-03,\n",
      "        -1.0119e-02,  1.9339e-02, -1.7922e-02, -2.8403e-03,  2.6848e-02,\n",
      "        -1.7417e-02,  1.3872e-02, -1.2178e-02, -1.3841e-02,  2.0613e-02,\n",
      "         2.0352e-02, -1.6787e-02, -1.5077e-02,  1.6743e-02,  1.2485e-02,\n",
      "        -2.6010e-02,  3.3249e-04, -5.0548e-03, -1.1064e-02,  1.6472e-02,\n",
      "         2.5520e-02, -1.6916e-03,  1.5031e-02, -1.6129e-02,  1.0348e-02,\n",
      "        -1.1234e-04, -3.8950e-02, -1.2720e-02, -1.8601e-03,  2.6810e-02,\n",
      "        -5.4161e-02,  3.9847e-03,  1.2088e-02,  2.6704e-02,  5.4112e-03,\n",
      "        -2.5344e-02,  5.5830e-03,  1.3830e-02,  7.1845e-03, -4.3604e-02,\n",
      "        -9.9835e-03, -4.4759e-03,  5.3945e-02,  1.4994e-03, -1.6534e-03,\n",
      "         5.2502e-04,  6.4354e-03, -1.6792e-02,  1.3424e-02, -9.6201e-03,\n",
      "        -1.3059e-04,  2.7632e-03, -1.9090e-02,  8.6977e-03,  9.5628e-03,\n",
      "        -2.2711e-02,  1.4177e-02, -3.0723e-02, -4.2081e-03, -1.2733e-02,\n",
      "         1.6013e-02,  1.7718e-02,  3.1935e-02,  7.6698e-03,  2.7986e-03,\n",
      "        -1.2239e-02,  1.2794e-02, -2.4807e-02,  8.2252e-03,  1.1988e-02,\n",
      "        -1.0991e-02, -6.4865e-03,  2.2967e-02,  1.0058e-02, -3.8354e-02,\n",
      "         1.2320e-02,  3.1962e-04,  6.2517e-03, -2.5813e-03,  2.6285e-02,\n",
      "         5.2423e-04, -1.5589e-02, -3.1902e-03, -7.2866e-03, -1.9327e-02,\n",
      "         1.7065e-02,  8.2033e-05,  5.3294e-03, -6.6873e-03, -2.8476e-03,\n",
      "        -1.4814e-02, -1.1373e-02, -7.3048e-03,  2.5512e-03,  1.3609e-02,\n",
      "        -3.4814e-02,  5.3507e-03,  6.9792e-03,  4.7845e-02,  1.1754e-02,\n",
      "        -1.3530e-02,  2.4771e-02,  4.6794e-02, -1.6206e-02, -4.4004e-02,\n",
      "         2.4754e-02, -8.5980e-03, -2.8718e-02,  8.1512e-03, -3.6138e-02,\n",
      "         2.9566e-02,  2.6253e-03, -2.4187e-03,  2.2305e-03, -2.5069e-02,\n",
      "         9.4491e-03, -2.5931e-02, -1.8893e-02,  1.0287e-02,  9.6928e-03,\n",
      "         4.2315e-02,  2.4783e-02,  7.1591e-03, -2.2227e-02, -2.1298e-02,\n",
      "        -2.5999e-02, -2.4673e-02, -9.1982e-03,  2.2357e-02, -1.3406e-02,\n",
      "         4.3042e-03, -2.4452e-02, -3.4168e-02, -2.8615e-02,  7.3774e-03,\n",
      "         2.8233e-02, -2.9364e-02,  3.4266e-02,  1.8980e-02, -1.3314e-02,\n",
      "         3.8320e-02, -5.0456e-02,  4.8403e-03,  1.2029e-02, -5.5905e-03,\n",
      "        -3.7340e-02,  6.1789e-03,  3.0815e-02,  4.1451e-03, -2.0121e-02,\n",
      "         8.7140e-03, -3.0568e-02, -4.3231e-02, -3.2499e-02, -1.8183e-02,\n",
      "        -1.1391e-02, -5.3851e-04,  7.2126e-03, -4.1586e-03,  3.4217e-02,\n",
      "         9.3953e-03,  5.0592e-03,  1.0638e-02, -4.5494e-02, -2.4568e-02,\n",
      "         9.0468e-03, -1.8717e-02,  4.5857e-03,  2.4281e-02, -2.6659e-02,\n",
      "        -3.8488e-02, -1.2423e-02,  2.3831e-02,  9.6451e-03, -1.6959e-03,\n",
      "        -2.0085e-02,  3.5268e-03, -5.7089e-03, -2.1944e-02,  1.4222e-02,\n",
      "         1.9463e-02,  3.8220e-03,  4.7485e-02,  4.2030e-03,  1.9575e-03,\n",
      "         9.8413e-03, -2.4814e-02, -2.9705e-02,  3.9811e-03, -1.4540e-03,\n",
      "         7.6712e-03,  4.9133e-02,  3.8828e-02,  8.4557e-03,  4.4117e-03,\n",
      "        -6.7543e-03,  1.1378e-02,  1.3988e-02, -1.1838e-02,  1.2081e-02,\n",
      "        -2.2100e-02, -5.3463e-04, -2.2322e-03, -4.4197e-03,  7.1645e-03,\n",
      "        -2.9384e-03, -1.3646e-02, -1.0035e-02,  2.2071e-02,  3.6764e-03,\n",
      "         3.5271e-03, -3.1749e-02,  2.7857e-02, -3.5243e-02,  2.2872e-02,\n",
      "        -1.5448e-02,  4.9467e-03, -2.0918e-03, -2.0330e-02,  1.5629e-02,\n",
      "        -1.9451e-02, -1.1141e-02,  6.2666e-03,  1.9953e-02, -1.5948e-02,\n",
      "         8.6057e-03, -9.6107e-03,  5.6842e-03, -2.3035e-02,  8.8647e-03,\n",
      "        -4.3369e-03, -3.5819e-02,  3.2054e-02,  1.8259e-02, -3.3333e-02,\n",
      "         1.0566e-02, -5.7521e-03,  7.6798e-03,  6.5608e-03, -1.2901e-02,\n",
      "        -1.8717e-02,  4.0461e-02,  1.6136e-02, -2.2549e-02,  4.1405e-02,\n",
      "         1.3884e-02, -1.5663e-02,  6.2712e-03,  8.5961e-03,  6.1051e-02,\n",
      "         3.7300e-02, -1.5643e-02, -1.3774e-02, -3.5622e-03, -1.4642e-03,\n",
      "         1.3799e-03, -3.0519e-02,  6.3878e-03,  3.6667e-02, -1.6874e-02,\n",
      "         6.1781e-04,  1.2824e-04,  1.6335e-02,  1.0012e-02,  1.9706e-02,\n",
      "         2.8520e-02, -1.9077e-02, -1.6424e-02], grad_fn=<SliceBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(len(tokenizer))  # 28996\n",
    "tokenizer.add_tokens([\"NEW_TOKEN\"])\n",
    "print(len(tokenizer))  # 28997\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "# The new vector is added at the end of the embedding matrix\n",
    "\n",
    "print(model.embeddings.word_embeddings.weight[-1, :])\n",
    "# Randomly generated matrix\n",
    "\n",
    "model.embeddings.word_embeddings.weight[-1, :] = torch.zeros([model.config.hidden_size])\n",
    "\n",
    "print(model.embeddings.word_embeddings.weight[-1, :])\n",
    "# outputs a vector of zeros of shape [768]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
